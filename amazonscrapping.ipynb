{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyforest in /opt/homebrew/lib/python3.11/site-packages (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from statsmodels.tsa.arima_model import ARIMA',\n",
       " 'import re',\n",
       " 'from sklearn.linear_model import Ridge',\n",
       " 'from sklearn import svm',\n",
       " 'from sklearn.ensemble import GradientBoostingRegressor',\n",
       " 'import sys',\n",
       " 'import seaborn as sns',\n",
       " 'from sklearn.linear_model import Lasso',\n",
       " 'import glob',\n",
       " 'from pyspark import SparkContext',\n",
       " 'from sklearn import metrics',\n",
       " 'import statsmodels.api as sm',\n",
       " 'import lightgbm as lgb',\n",
       " 'from sklearn.linear_model import ElasticNetCV',\n",
       " 'from sklearn.decomposition import PCA',\n",
       " 'from sklearn.model_selection import GridSearchCV',\n",
       " 'import cv2',\n",
       " 'from sklearn.ensemble import RandomForestRegressor',\n",
       " 'from sklearn.manifold import TSNE',\n",
       " 'from sklearn.model_selection import StratifiedKFold',\n",
       " 'from sklearn.feature_extraction.text import TfidfVectorizer',\n",
       " 'from sklearn.model_selection import RandomizedSearchCV',\n",
       " 'from sklearn.cluster import KMeans',\n",
       " 'from sklearn.preprocessing import StandardScaler',\n",
       " 'import os',\n",
       " 'import plotly.graph_objs as go',\n",
       " 'from sklearn.preprocessing import MinMaxScaler',\n",
       " 'import awswrangler as wr',\n",
       " 'from sklearn.linear_model import LassoCV',\n",
       " 'import matplotlib as mpl',\n",
       " 'import pandas as pd',\n",
       " 'from sklearn.impute import SimpleImputer',\n",
       " 'from sklearn.preprocessing import OneHotEncoder',\n",
       " 'import pickle',\n",
       " 'import skimage',\n",
       " 'from sklearn.model_selection import KFold',\n",
       " 'import gensim',\n",
       " 'from fbprophet import Prophet',\n",
       " 'import dash',\n",
       " 'from sklearn.model_selection import cross_val_score',\n",
       " 'import nltk',\n",
       " 'from scipy import stats',\n",
       " 'from sklearn.preprocessing import RobustScaler',\n",
       " 'from sklearn.linear_model import LogisticRegression',\n",
       " 'import tensorflow as tf',\n",
       " 'from sklearn.feature_extraction.text import CountVectorizer',\n",
       " 'from PIL import Image',\n",
       " 'from sklearn.ensemble import RandomForestClassifier',\n",
       " 'import numpy as np',\n",
       " 'import altair as alt',\n",
       " 'import textblob',\n",
       " 'import matplotlib.pyplot as plt',\n",
       " 'from sklearn.linear_model import RidgeCV',\n",
       " 'import imutils',\n",
       " 'from scipy import signal as sg',\n",
       " 'import xgboost as xgb',\n",
       " 'import sklearn',\n",
       " 'from sklearn.preprocessing import PolynomialFeatures',\n",
       " 'import pydot',\n",
       " 'import fbprophet',\n",
       " 'import plotly as py',\n",
       " 'from sklearn.linear_model import ElasticNet',\n",
       " 'from openpyxl import load_workbook',\n",
       " 'from sklearn.linear_model import LinearRegression',\n",
       " 'import datetime as dt',\n",
       " 'from sklearn.preprocessing import LabelEncoder',\n",
       " 'import spacy',\n",
       " 'from sklearn.model_selection import train_test_split',\n",
       " 'import fastai',\n",
       " 'from xlrd import open_workbook',\n",
       " 'from pathlib import Path',\n",
       " 'from dask import dataframe as dd',\n",
       " 'import bokeh',\n",
       " 'import statistics',\n",
       " 'import keras',\n",
       " 'import plotly.express as px',\n",
       " 'from sklearn.ensemble import GradientBoostingClassifier',\n",
       " 'import tqdm',\n",
       " 'import torch']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyforest import *\n",
    "lazy_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/homebrew/lib/python3.11/site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /opt/homebrew/lib/python3.11/site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/homebrew/lib/python3.11/site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/homebrew/lib/python3.11/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/anurag2005/Library/Python/3.11/lib/python/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /opt/homebrew/lib/python3.11/site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /Users/anurag2005/Library/Python/3.11/lib/python/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/anurag2005/Library/Python/3.11/lib/python/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/homebrew/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/homebrew/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /opt/homebrew/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/anurag2005/Library/Python/3.11/lib/python/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/anurag2005/Library/Python/3.11/lib/python/site-packages (from beautifulsoup4->bs4) (2.4.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromedriver_binary==122.0.6261.111 in /opt/homebrew/lib/python3.11/site-packages (122.0.6261.111.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chromedriver_binary==122.0.6261.111\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://www.amazon.com/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_url(keyword):\n",
    "    template = 'https://www.amazon.com/s?k={}&ref=nb_sb_noss_1'\n",
    "    keyword = keyword.replace(' ','+')\n",
    "    return template.format(keyword)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amazon.com/s?k=Srimad+Bhagavatam&ref=nb_sb_noss_1'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url= my_url('Srimad Bhagavatam')\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "soup = bs4.BeautifulSoup(driver.page_source,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_results=soup.find_all('div',{'data-component-type':'s-search-result'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=soup_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "atag = obj.h2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = atag.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's now create a generic url\n",
    "\n",
    "url='https://www.amazon.com/'+atag.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$13.99'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent=obj.find('span','a-price')\n",
    "\n",
    "price=parent.find('span','a-offscreen').text\n",
    "\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from bs4 import BeautifulSoup\n",
    "#for moicrosoft edge\n",
    "\n",
    "import csv\n",
    "\n",
    "#We will be using functions to achieve this\n",
    "\n",
    "def my_url(keyword):\n",
    "    temp = 'https://www.amazon.com/s?k={}&ref=nb_sb_noss_1'\n",
    "    keyword = keyword.replace(' ', '+')\n",
    "    \n",
    "    # Add Term Query To URL\n",
    "    url = temp.format(keyword)\n",
    "    \n",
    "    # Add Page Query Placeholder\n",
    "    url += '&page{}'\n",
    "    \n",
    "    return url\n",
    "\n",
    "def extract_record(obj):\n",
    "    atag = obj.h2.a\n",
    "    description = atag.text.strip()\n",
    "    url = 'https://www.amazon.com' + atag.get('href')\n",
    "    \n",
    "    #it is possible that some items on amazom.com might not be having one of the items we are looking for(e.g. some items might not be having ratings or price), we will be getting error if we dont take care of that. We will therefore add some error handlers\n",
    "    #if there are no price,probably the item is out of stock or not available, then we will ignore the item, but if there are no reviews yet, it's fine, we will still want to extract the item.\n",
    "    try:\n",
    "        parent=obj.find('span','a-price')\n",
    "        price=parent.find('span','a-offscreen').text\n",
    "    except AttributeError: #we are excepting the error if it occurs so that we can move to extract the next item, else the program will stop running and gives error\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        rate=obj.i.text\n",
    "        counts_review = obj.find('span', {'class': 'a-size-base', 'dir': 'auto'}).text\n",
    "    except AttributeError:\n",
    "        #assigning empty string to ratings and \n",
    "        rate = ''\n",
    "        counts_review = ''\n",
    "    \n",
    "    image = obj.find('img', {'class': 's-image'}).get('src') \n",
    "    \n",
    "    #let's create a tuple that will contain all these items and assign it to a result variable\n",
    "    result = (description, price, rate, counts_review, url,image)\n",
    "    return result\n",
    "\n",
    "'''Run Main Program Routine'''\n",
    "def main(keyword):\n",
    "    # Startup The Webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "#     options = EdgeOptions()\n",
    "#     options.use_chromium =True\n",
    "#     driver = Edge(options=options)\n",
    "    \n",
    "    records = []  #an empty records list to contain all of our extracted records\n",
    "    url =my_url(keyword)\n",
    "    \n",
    "    for page in range(1, 50):\n",
    "        driver.get(url.format(page))\n",
    "        soup =BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        results=soup.find_all('div',{'data-component-type':'s-search-result'})\n",
    "#         results=soup.find_all('div',{'data-component-type': 's-search-result'}) #same as we did above\n",
    "\n",
    "        \n",
    "#we will like to check if what we have return from the extract_record function is empty or not\n",
    "        for item in results:\n",
    "            record = extract_record(item) \n",
    "            if record: #if the record has something in it append to records list\n",
    "                records.append(record) \n",
    "                \n",
    "#         driver.quit()\n",
    "    \n",
    "#     # Save Results To CSV File\n",
    "        with open('Results.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Description', 'Price', 'Rating', 'Reviews Count', 'URL','Image link'])\n",
    "            writer.writerows(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('school bag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Documents/Results.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
